import datasets
from sklearn.cluster import KMeans
import argparse
import os

def get_train_dict(example):
    train_dict[example['id']]=example['vector']

def gen_sen_id_field(example):
    example['semantic_id']=''
    return example


def Generate_Semantic_ids(goal_dataset,doc_embeddings, fd='',num_cluster=10, c=100):
    global final_data
    final_data=fd
    km=KMeans(n_clusters=num_cluster).fit(doc_embeddings)
    def add_new_cluster_id(example,idx):
        example['semantic_id']+=str(km.labels_[idx])
        return example
    train_data=goal_dataset.map(add_new_cluster_id, with_indices=True)
    for i in range(num_cluster):
        km_i=train_data.filter(lambda example: example['semantic_id'][-1]==str(i))
        if len(km_i)<c:
            if len(km_i)==0:
                continue
            elif len(km_i)!=1:
                def add_final_id(example,idx):
                    example['semantic_id'] +=str(idx)
                    return example
                km_i=km_i.map(add_final_id, with_indices=True)
            if final_data=='':
                final_data=km_i
            else:
                final_data=datasets.concatenate_datasets([final_data,km_i])
            print(f'processed_num:{len(final_data)}')
        else:
            Generate_Semantic_ids(km_i,km_i['vector'],final_data, num_cluster, c)


def map_random_id_to_semantic_id(example):
    random_id=example['id'].replace('\n','')
    semantic_id=example['semantic_id']
    map_dict[random_id]=semantic_id




if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--embedding_path",
                        default=None,
                        type=str,
                        required=True,
                        help="The embedding file of the corpus. The embeddings are usually generated by language models "
                             "such as BERT. The format of the file should be .jsonl")
    parser.add_argument("--output_dir",
                        default=None,
                        type=str,
                        required=True,
                        help="The output dir of the semantic id file. This should be a directory.")
    parser.add_argument("--num_cluster",
                        default=10,
                        type=int,
                        help="The cluster number of each step when performing K-Means.")
    parser.add_argument("--cluster_size",
                        default=100,
                        type=int,
                        help="The max number of docs in final cluster.")
    args = parser.parse_args()
    print('The args: {}'.format(args))
    vector_path = args.embedding_path
    save_map_dir = os.path.join(args.output_dir, 'semantic_id_map.pkl')
    print(f'input_path:{vector_path}, \n'
          f'output_path:{save_map_dir}')
    train_data = datasets.load_dataset('json', data_files=vector_path, ignore_verifications=False)['train']
    train_dict = {}

    train_data.map(get_train_dict)
    train_data = train_data.map(gen_sen_id_field)

    Generate_Semantic_ids(train_data, train_data['vector'], num_cluster=args.num_cluster, c=args.cluster_size)
    print(f'len semantic ids:{len(final_data)}')

    map_dict = {}
    final_data.map(map_random_id_to_semantic_id)

    import pickle

    with open(save_map_dir, 'wb') as outf:
        pickle.dump(map_dict, outf)

